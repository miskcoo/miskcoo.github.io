---
title: Yuhao Zhou | Home
nav-title: Home
---
I am a Ph.D. student (from fall, 2020) in the Department of Computer Science and Technology at [Tsinghua University](https://www.tsinghua.edu.cn/en/), advised by [Prof. Jun Zhu](http://ml.cs.tsinghua.edu.cn/~jun). 

I received my bachelor degree at Tsinghua University, majored in _Computer Science and Technology_. 
I also received my second bachelor degree at Tsinghua University, majored in _Pure and Applied Mathematics_.

You can find me at [Twitter](https://twitter.com/miskcoo), [Google Scholar](https://scholar.google.com/citations?user=GKLRbxoAAAAJ&hl=en), and [GitHub](http://github.com/miskcoo/)!

## Publications

* **Scalable Quasi-Bayesian Inference for Instrumental Variable Regression**{:.paper-title}  
  Ziyu Wang\*, **Yuhao Zhou**{:.author-me}\*, Tongzheng Ren, Jun Zhu.  
  Short version to appear in _NeurIPS 2021_.  
  [[short version]](https://ml.cs.tsinghua.edu.cn/~jun/pub/quasi-bayes.pdf)
  [[full version]](https://arxiv.org/abs/2106.08750) 
  [[code]](https://github.com/meta-inf/qbdiv)
  [[slides]](https://ml.cs.tsinghua.edu.cn/~ziyu/static/p/qbdiv/slides-1.pdf)
  [[abstract]](javascript:void(0);)

  {:.paper-abstract .paper-toggle}
  **Abstract**: Recent years have witnessed an upsurge of interest in employing flexible machine learning models for instrumental variable (IV) regression, but the development of uncertainty quantification methodology is still lacking. In this work we present a scalable quasi-Bayesian procedure for IV regression, building upon the recently developed kernelized IV models. Contrary to Bayesian modeling for IV, our approach does not require additional assumptions on the data generating process, and leads to a scalable approximate inference algorithm with time cost comparable to the corresponding point estimation methods. Our algorithm can be further extended to work with neural network models. We analyze the theoretical properties of the proposed quasi-posterior, and demonstrate through empirical evaluation the competitive performance of our method. 

* **Nonparametric Score Estimators**{:.paper-title}  
  **Yuhao Zhou**{:.author-me}, Jiaxin Shi, Jun Zhu.  
  _International Conference on Machine Learning (ICML)_, 2020.  
  [[pdf]](http://proceedings.mlr.press/v119/zhou20c.html) 
  [[arxiv]](https://arxiv.org/abs/2005.10099) 
  [[code]](https://github.com/miskcoo/kscore) 
  [[slides]](https://ml.cs.tsinghua.edu.cn/~yuhao/slides/nonparametric score estimators, icml2020.pdf)
  [[abstract]](javascript:void(0);)

  {:.paper-abstract .paper-toggle}
  **Abstract**: Estimating the score, i.e., the gradient of log density function, from a set of samples generated by an unknown distribution is a fundamental task in inference and learning of probabilistic models that involve flexible yet intractable densities. Kernel estimators based on Stein's methods or score matching have shown promise, however their theoretical properties and relationships have not been fully-understood. We provide a unifying view of these estimators under the framework of regularized nonparametric regression. It allows us to analyse existing estimators and construct new ones with desirable properties by choosing different hypothesis spaces and regularizers. A unified convergence analysis is provided for such estimators. Finally, we propose score estimators based on iterative regularization that enjoy computational benefits from curl-free kernels and fast convergence.

  {:.paper-bibtex .paper-toggle}
  ```plain
  @article{zhou2020nonparametric,
	  title={Nonparametric Score Estimators},
	  author={Zhou, Yuhao and Shi, Jiaxin and Zhu, Jun},
	  journal={arXiv preprint arXiv:2005.10099},
	  year={2020}
  }
  ```

### Preprints & Other Abstracts

* **A Semismooth Newton based Augmented Lagrangian Method for Nonsmooth Optimization on Matrix Manifolds**{:.paper-title}  
  **Yuhao Zhou**{:.author-me}, Chenglong Bao, Chao Ding, Jun Zhu.  
  _Preprint. Arxiv:2103.02855._  
  [[arxiv]](https://arxiv.org/abs/2103.02855) 
  [[abstract]](javascript:void(0);)

  {:.paper-abstract .paper-toggle}
  **Abstract**: This paper is devoted to studying an augmented Lagrangian method for solving a class of manifold optimization problems, which have nonsmooth objective functions and non-negative constraints. Under the constant positive linear dependence condition on manifolds, we show that the proposed method converges to a stationary point of the nonsmooth manifold optimization problem. Moreover, we propose a globalized semismooth Newton method to solve the augmented Lagrangian subproblem on manifolds efficiently. The local superlinear convergence of the manifold semismooth Newton method is also established under some suitable conditions. We also prove that the semismoothness on submanifolds can be inherited from that in the ambient manifold. Finally, numerical experiments on compressed modes and (constrained) sparse principal component analysis illustrate the advantages of the proposed method.

* **Spectral Estimators for Gradient Fields of Log-Densities**{:.paper-title}  
  **Yuhao Zhou**{:.author-me}, Jiaxin Shi, Jun Zhu.  
  _[ICML Workshop on Steinâ€™s Method](https://steinworkshop.github.io/)_, Long Beach, USA, 2019.
