---
title: Yuhao Zhou | Home
nav-title: Home
---
I am a Ph.D. student (from fall, 2020) in the Department of Computer Science and Technology at [Tsinghua University](https://www.tsinghua.edu.cn/en/), advised by [Prof. Jun Zhu](http://ml.cs.tsinghua.edu.cn/~jun). 

I received my bachelor degree at Tsinghua University, majored in _Computer Science and Technology_. 
I also received my second bachelor degree at Tsinghua University, majored in _Pure and Applied Mathematics_.

You can find me at [Twitter](https://twitter.com/miskcoo), [Google Scholar](https://scholar.google.com/citations?user=GKLRbxoAAAAJ&hl=en), and [GitHub](http://github.com/miskcoo/)!

## Publications

### Conference Papers

* **Nonparametric Score Estimators**{:.paper-title}  
  **Yuhao Zhou**{:.author-me}, Jiaxin Shi, Jun Zhu.  
  _International Conference on Machine Learning (ICML)_, 2020.  
  [[arxiv]](https://arxiv.org/abs/2005.10099) 
  [[code]](https://github.com/miskcoo/kscore) 
  [[slides]](http://ml.cs.tsinghua.edu.cn/~yuhao/slides/nonparametric score estimators, icml2020.pdf)
  [[abstract]](javascript:void(0);)

  {:.paper-abstract .paper-toggle}
  **Abstract**: Estimating the score, i.e., the gradient of log density function, from a set of samples generated by an unknown distribution is a fundamental task in inference and learning of probabilistic models that involve flexible yet intractable densities. Kernel estimators based on Stein's methods or score matching have shown promise, however their theoretical properties and relationships have not been fully-understood. We provide a unifying view of these estimators under the framework of regularized nonparametric regression. It allows us to analyse existing estimators and construct new ones with desirable properties by choosing different hypothesis spaces and regularizers. A unified convergence analysis is provided for such estimators. Finally, we propose score estimators based on iterative regularization that enjoy computational benefits from curl-free kernels and fast convergence.

  {:.paper-bibtex .paper-toggle}
  ```plain
  @article{zhou2020nonparametric,
	  title={Nonparametric Score Estimators},
	  author={Zhou, Yuhao and Shi, Jiaxin and Zhu, Jun},
	  journal={arXiv preprint arXiv:2005.10099},
	  year={2020}
  }
  ```

### Workshop Papers & Other Abstracts

* **A Semi-smooth Newton based Augmented Lagrangian Method for Nonsmooth Optimization on Matrix Manifolds**{:.paper-title}  
  **Yuhao Zhou**{:.author-me}, Chenglong Bao, Chao Ding, Jun Zhu.  
  _Preprint. Arxiv:2103.02855._  
  [[arxiv]](https://arxiv.org/abs/2103.02855) 
  [[abstract]](javascript:void(0);)

  {:.paper-abstract .paper-toggle}
  This paper is devoted to studying an inexact augmented Lagrangian method for solving a class of manifold optimization problems, which have non-smooth objective functions and non-negative constraints. Under the constant positive linear dependence condition on manifold, we show that the proposed method converges to a stationary point of the non-smooth manifold optimization problem. Moreover, we propose a globalized semi-smooth Newton method to solve the augmented Lagrangian subproblem on manifolds efficiently. The local superlinear convergence of the manifold semi-smooth Newton method is also established under some suitable conditions. Finally, numerical experiments on compressed modes and (constrained) sparse PCA illustrate the advantages of the proposed method in terms of accuracy and computational efficiency.

* **Spectral Estimators for Gradient Fields of Log-Densities**{:.paper-title}  
  **Yuhao Zhou**{:.author-me}, Jiaxin Shi, Jun Zhu.  
  _[ICML Workshop on Steinâ€™s Method](https://steinworkshop.github.io/)_, Long Beach, USA, 2019.
